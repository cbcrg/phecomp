/*
#################################################################################
### Jose Espinosa-Carrasco. CB/CSN-CRG. April 2015                            ### 
#################################################################################
### Code : 23.04                                                              ###
### Position from tac files                                                   ### 
### ./nextflow tac2activity.nxf --tac_files '*.tac'                           ###
#################################################################################
*/

params.tac_files = "/phecomp/data/CRG/20120502_FDF_CRG/20120502_FDF_CRG/test/*.tac"

tac_files_path = "$HOME/${params.tac_files}"

println "path: $tac_files_path"

tac_files = Channel.fromPath(tac_files_path)

path_tac2pos = "$HOME/git/phecomp/lib/c/"

correspondence_f_path = "$HOME/git/pergola/test/posInt2pergola.txt"
correspondence_f = file(correspondence_f_path)

// Correspondence file for bedGraph files
correspondence_f_bG_path = "$HOME/git/pergola/test/bedGraph2pergola.txt"
correspondence_f_bG = file(correspondence_f_bG_path)

correspondence_f_bG_tr_path = "$HOME/git/pergola/test/bedGraphWithTr2pergola.txt"
correspondence_f_bG_tr = file(correspondence_f_bG_tr_path)

/* 
 * Creating results folder
 */
dump_dir = file("$HOME/phecomp/processedData/201205_FDF_CRG/tac2activity/")

dump_dir.with {
     if( !empty() ) { deleteDir() }
     mkdirs()
     println "Created: $dump_dir"
}

/*
 * Extract postion from tac files
 * Filtering or zero values, files smaller
 * Filtering all intervals before 1335985200 (First 8 PM ocurrence, start of dark phase)
 * Eventually set it as a variable for other data sets #TODO
 */
process extractPosition {
 
 input:
 file tac from tac_files
 
 // I duplicate the output, one for writing file and one for continue pipe
 output:
// set '*.pos' into act_file
 file '*.pos' into pos_files1
 file '*.pos' into pos_files2
 file 'min_max.txt' into min_max
 
 script:
 println "Input name is $tac.name"
   
 """
 ${path_tac2pos}new_tac2pos -file $tac -action position > ${tac}.txt
 
 name_file=${tac}.txt
 ini_name="\${name_file:0:8}" 
 if [[ "\${ini_name}" = "20120502" ]] ; then
    cat ${tac}.txt | head -1 > ${tac}.pos
    # tail -n +2 ${tac}.txt | awk -F ";" '{ if (\$7 != 0) print; }' | awk -F  ";" '{ if (\$3 > 1335985200) print; }' >> ${tac}.pos
    # I eliminate all zeros, files much smaller, two awk, otherwise to much memory
    #tail -n +2 ${tac}.txt | awk -F ";" '{ if (\$7 != 0) print; }' > temp.txt
    #awk -F  ";" '{ if (\$3 > 1335985200) print; }' temp.txt >> ${tac}.pos
    tail -n +2 ${tac}.txt | awk -F  ";" '{ if (\$3 > 1335985200) print; }' >> ${tac}.pos
 else
    #cat ${tac}.txt | head -1 > ${tac}.pos
    #tail -n +2 ${tac}.txt | awk -F ";" '{ if (\$7 != 0) print; }' >> ${tac}.pos
    cp ${tac}.txt ${tac}.pos
    echo -e "No changes in this files\\n"
    tail -n +2 ${tac}.pos | awk -F ";"  'BEGIN{OFS=";";} { if (min=="") { min=max=\$3 }; if(\$3>max){ max=\$3 }; if(\$3< min) { min=\$3 }; total+=\$3; count+=1 } END { print "1",min,"0",max }' >> min_max.txt
 fi
 """

}

/*
Another way of duplicating the channels
pos_files1 = Channel.create() 
pos_files2 = Channel.create()
pos_files.into(pos_files1, pos_files2) 
 */

/*
 * Writing position file
 */ 

pos_files1
    .subscribe {
        println "Copying pos file: $it"
        it.copyTo( dump_dir.resolve ( it.name ) )
  }

  
/*
 * Join position files
 * Puedo hacer los archivos uno a uno y luego unirlos dejando siempre la coordinadas como las originales
 * Se puede unir despues y hacer luego la coordinadas relativas???
 * Puedo leer directamente el bed o el bed graph no deja de ser un archivo csv
 */
// /* del
pos_files2Flat = pos_files2.flatten().map { single_pos_file ->   
   def name = single_pos_file.name
   def content = single_pos_file
   [ name,  content ]
}

/*
 * I duplicate the channel because I will use it to generate both bed and bedGraph files
 */
pos_files2Flat_bed = Channel.create() 
pos_files2Flat_bG = Channel.create()
pos_files2Flat.into(pos_files2Flat_bG, pos_files2Flat_bed) 

/*
 * Generating bedGraph files from position files
 */
process pos_to_bedGraph {
    
    input: 
    set val (f_pos_name), file ('f_pos') from pos_files2Flat_bG
    
    output:
    set val(f_pos_name), file('*.bedGraph') into bedGraph
    set val(f_pos_name), file('*.bedGraph') into bedGraph2

    // Command example
    //pergola_rules.py -i "/Users/jespinosa/phecomp/processedData/201205_FDF_CRG/tac2activity/20120502_FDF_CRG_c6.tac.pos" -o "/Users/jespinosa/git/pergola/test/position2pergola.txt" -fs ";" -nt

    """ 
    pergola_rules.py -i $f_pos -o $correspondence_f -fs ";" -f bedGraph -nt
    """
}

bedGraph2 
    .subscribe { pos_file, bedGraph_files ->   
        for ( it in bedGraph_files ) {
            it.copyTo( dump_dir.resolve ( "${pos_file}${it.name}" ) )
            }  
    }

// Collects here just get after flatting the correspondece bw
// position and bedgraph files 
// pos1; tr_1.bedGraph
// pos1; tr_2.bedGraph...
// pos2; tr_1.bedGraph
// pos2; tr_2.bedGraph...
// ...
// posN; tr_1.bedGraph
// posN; tr_2.bedGraph...
// posN; tr_n.bedGraph  
    
bedGraph_tr = bedGraph.map {pos_file, bedGraph_files ->
        bedGraph_files .collect {
            def pattern = it.name =~/^tr_(\d+).*$/
            def track = pattern[0][1]

            [ pos_file, track,  it ]
        }
    }
    .flatMap()
//    .println()

// Here with collect file as bedgraph files have the same name I can
// group them by this name and of course their content
bG_by_track = bedGraph_tr    
    .collectFile { pos, track, file -> 
       [ "bedGraph_$track", file ]
    }
   .flatMap() 
   .map { file -> tuple( file, file.baseName.tokenize('_')[1]) }
//   .println() 

process add_track2file {
    input:
    set file ('bedGraph_f'), val(tr) from bG_by_track
    
    output:
    set file ('*.txt'), val(tr) into bG_by_track_col
    
    
    """
    awk '{OFS="\t"; print \$1,\$2,\$3,\$4, ${tr}}' ${bedGraph_f} > tr_${tr}.txt   
    """        
}

/*
bG_by_track_col
    .println()
*/

process bG_to_relative_coord {       

    input:
    set file ('bedGraph_f'), val(tr) from bG_by_track_col
    
    output:
    set file ('*.bedGraph'), val(tr) into bG_rel_coord
    
    """ 
    pergola_rules.py -i $bedGraph_f -o $correspondence_f_bG_tr -f bedGraph -nh -s 'chrm' 'start' 'end' 'value' 'tr' -e 
    """ 
    
}
       
bG_rel_coord
    .subscribe  {  
        println "Copying %%%%%%%%%%%%%: ${it[1]}_relCoord.bedGraph"
        it[0].copyTo( dump_dir.resolve ( "tr_${it[1]}_relCoord.bedGraph" ) )
    }  

////////////////////////////

/*
 * Generating bed files from position files
 */
process pos_to_bed {
    
    input: 
    set val (f_pos_name), file ('f_pos') from pos_files2Flat_bed
    
    output:
    set val(f_pos_name), file('tr*.bed') into bed
    set val(f_pos_name), file('tr*.bed') into bed2

    // Command example -nt no track line
    //pergola_rules.py -i "/Users/jespinosa/phecomp/processedData/201205_FDF_CRG/tac2activity/20120502_FDF_CRG_c6.tac.pos" -o "/Users/jespinosa/git/pergola/test/position2pergola.txt" -fs ";" -nt

    """ 
    pergola_rules.py -i $f_pos -o $correspondence_f -fs ";" -f bed -nt
    """
}

bed2 
    .subscribe { pos_file, bed_files ->   
        for ( it in bed_files ) {
            it.copyTo( dump_dir.resolve ( "${pos_file}${it.name}" ) )
            }  
    }

// Collects here just get after flatting the correspondece bw
// position and bed files 
// pos1; tr_1.bed
// pos1; tr_2.bed...
// pos2; tr_1.bed
// pos2; tr_2.bed...
// ...
// posN; tr_1.bed
// posN; tr_2.bed...
// posN; tr_n.bed  
    
bed_tr = bed.map {pos_file, bed_files ->
        bed_files .collect {
            def pattern = it.name =~/^tr_(\d+).*$/
            def track = pattern[0][1]

            [ pos_file, track,  it ]
        }
    }
    .flatMap()
//    .println()

// Here with collect file as bedgraph files have the same name I can
// group them by this name and of course their content
bed_by_track = bed_tr    
    .collectFile { pos, track, file -> 
       [ "bed_$track", file ]
    }
   .flatMap() 
   .map { file -> tuple( file, file.baseName.tokenize('_')[1]) }
//   .println() 

process add_track2file {
    input:
    set file ('bed_f'), val(tr) from bed_by_track
    
    output:
    set file ('*.txt'), val(tr) into bed_by_track_col
    
    
    """
    awk '{OFS="\t"; print ${tr},\$2,\$3,\$4,\$5,\$6,\$7,\$8,\$9}' ${bed_f} > tr_${tr}.txt   
    """        
}

/*
bG_by_track_col
    .println()
*/

correspondence_f_path_bed = "$HOME/git/pergola/test/bed2pergola.txt"
correspondence_f_bed = file(correspondence_f_path_bed)

process bed_to_relative_coord {       

    input:
    set file ('bed_f'), val(tr) from bed_by_track_col
    
    output:
    set file ('tr*.bed'), val(tr) into bed_rel_coord
    
    """ 
    pergola_rules.py -i $bed_f -o $correspondence_f_bed -f bed -nh -s 'chrm' 'start' 'end' 'nature' 'value' 'strain' 'start_rep' 'end_rep' 'color' -e 
    """ 
    
}
       
bed_rel_coord
    .subscribe  {  
        println "Copying %%%%%%%%%%%%%: ${it[1]}_relCoord.bed"
        it[0].copyTo( dump_dir.resolve ( "tr_${it[1]}_relCoord.bed" ) )
    } 
    
/*
 * Getting phase files from min_max 
 * I filtered all data before 8PM that is why I have to reverse light and dark
 * phases
 */  
process get_phases {
    input:
    file min_max from mix_max_joined
 
    output:
    set file('phases_light_inverted.bed') into light_phases
    set file('phases_dark_inverted.bed') into dark_phases
    set file('phases_inverted.bed') into all_phases
    set file('phases_light_inverted.bed') into light_phases_p
    set file('phases_dark_inverted.bed') into dark_phases_p
    
    """ 
    pergola_rules.py -i $min_max -o $correspondence_f -fs ";" -f bed -nh -s 'Cage' 'Time' 'EucDistance' 'TimeEnd' -e
    sed 's/dark/light/g' phases_dark.bed > phases_light_inverted.bed
    sed 's/light/dark/g' phases_light.bed > phases_dark_inverted.bed
    sed 's/light/da1rk/g' phases.bed | sed 's/dark/light/g' | sed 's/da1rk/dark/g' > phases_inverted.bed
    
    """ 
    }
    
/*
 * Writing all phases file for its display in the genome browser
 */
/*
all_phases
 .subscribe {
        println "Phases file saved to file: $it"        
        it.copyTo( dump_dir.resolve ( it.name ) )
    }
